{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eff9cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading TensorFlow Model ---\n",
      "âœ… Model loaded successfully.\n",
      "\n",
      "--- Step 1: Compressing 'D:\\visionpipeline\\sampledata\\example2.mp4' ---\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['ffmpeg', '-i', 'D:\\\\visionpipeline\\\\sampledata\\\\example2.mp4', '-vf', 'scale=-1:480', '-an', '-vcodec', 'libx264', '-crf', '28', 'compressed_videos\\\\compressed_example2.mp4', '-y']' returned non-zero exit status 3752568763.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 117\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Step 1: Compressing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_video\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m command \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m'\u001b[39m, input_video, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-vf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale=-1:480\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-an\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-vcodec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibx264\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-crf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28\u001b[39m\u001b[38;5;124m'\u001b[39m, compressed_video, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-y\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    116\u001b[0m ]\n\u001b[1;32m--> 117\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVNULL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVNULL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Compression complete. Processing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompressed_video\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m video_path \u001b[38;5;241m=\u001b[39m compressed_video\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.10.19-windows-x86_64-none\\lib\\subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[1;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    527\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['ffmpeg', '-i', 'D:\\\\visionpipeline\\\\sampledata\\\\example2.mp4', '-vf', 'scale=-1:480', '-an', '-vcodec', 'libx264', '-crf', '28', 'compressed_videos\\\\compressed_example2.mp4', '-y']' returned non-zero exit status 3752568763."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. SETUP, CONSTANTS & MODEL LOADING ---\n",
    "\n",
    "# --- Model and Processing Constants ---\n",
    "MODEL_PATH = 'best_model.h5'  # <--- IMPORTANT: SET YOUR MODEL FILENAME HERE\n",
    "SEQUENCE_LENGTH = 150\n",
    "NUM_FEATURES = 66\n",
    "\n",
    "# --- Directory Setup ---\n",
    "COMPRESSED_FOLDER = \"compressed_videos\"\n",
    "OUTPUT_FOLDER = \"output_videos\"\n",
    "os.makedirs(COMPRESSED_FOLDER, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# --- Load the Trained GRU Model ---\n",
    "print(\"--- Loading TensorFlow Model ---\")\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "print(\"âœ… Model loaded successfully.\")\n",
    "\n",
    "# --- MediaPipe Initialization ---\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# --- 2. HELPER FUNCTIONS ---\n",
    "\n",
    "def draw_body_landmarks(image, landmarks):\n",
    "    \"\"\"Draws the body skeleton on an image.\"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    # Define connections between keypoints to form a skeleton\n",
    "    pairs = [\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n",
    "        (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n",
    "        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),\n",
    "        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST),\n",
    "        (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE),\n",
    "        (mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE),\n",
    "        (mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE),\n",
    "        (mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE),\n",
    "    ]\n",
    "    # Draw lines for each pair of connected keypoints\n",
    "    for a, b in pairs:\n",
    "        pa, pb = landmarks.landmark[a.value], landmarks.landmark[b.value]\n",
    "        # Only draw if both points are reasonably visible\n",
    "        if pa.visibility > 0.5 and pb.visibility > 0.5:\n",
    "            cv2.line(image, (int(pa.x * w), int(pa.y * h)), (int(pb.x * w), int(pb.y * h)), (255, 0, 0), 2)\n",
    "            cv2.circle(image, (int(pa.x * w), int(pa.y * h)), 4, (0, 0, 255), -1)\n",
    "            cv2.circle(image, (int(pb.x * w), int(pb.y * h)), 4, (0, 0, 255), -1)\n",
    "    return image\n",
    "\n",
    "def draw_prediction_info(image, status, confidence):\n",
    "    \"\"\"Draws the prediction status and confidence on the image.\"\"\"\n",
    "    color = (0, 255, 0) if status == \"Correct\" else (0, 0, 255)\n",
    "    \n",
    "    # Status box\n",
    "    cv2.rectangle(image, (0, 0), (320, 60), (245, 117, 16), -1)\n",
    "    \n",
    "    # Display Status\n",
    "    cv2.putText(image, 'STATUS', (15, 20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, status, (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display Confidence\n",
    "    cv2.putText(image, 'CONF', (220, 20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, f'{confidence:.2%}', (210, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_video_resolution(video_path):\n",
    "    \"\"\"Gets video resolution using ffprobe.\"\"\"\n",
    "    command = ['ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
    "               '-show_entries', 'stream=width,height', '-of', 'json', video_path]\n",
    "    try:\n",
    "        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        info = json.loads(result.stdout)\n",
    "        return info['streams'][0]['width'], info['streams'][0]['height']\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"Error: ffprobe not found. Please ensure ffmpeg is installed and in your system's PATH.\")\n",
    "        return None, None\n",
    "\n",
    "# --- 3. USER INPUT ---\n",
    "\n",
    "choice = input(\"Enter 'v' for video upload or 'w' for webcam: \").lower().strip()\n",
    "\n",
    "# --- 4. VIDEO vs WEBCAM LOGIC ---\n",
    "\n",
    "if choice == 'v':\n",
    "    input_video = input(\"Enter the full path to your video file: \").strip().strip('\"') # Strip quotes for drag-and-drop\n",
    "    if not os.path.exists(input_video):\n",
    "        raise FileNotFoundError(f\"The file was not found at: {input_video}\")\n",
    "\n",
    "    base_filename = os.path.basename(input_video)\n",
    "    compressed_video = os.path.join(COMPRESSED_FOLDER, f\"compressed_{base_filename}\")\n",
    "    output_filename = os.path.join(OUTPUT_FOLDER, f\"output_{base_filename}\")\n",
    "\n",
    "    print(f\"\\n--- Step 1: Compressing '{input_video}' ---\")\n",
    "    command = [\n",
    "        'ffmpeg', '-i', input_video, '-vf', 'scale=-1:480', '-an', \n",
    "        '-vcodec', 'libx264', '-crf', '28', compressed_video, '-y'\n",
    "    ]\n",
    "    subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    print(f\"âœ… Compression complete. Processing '{compressed_video}'...\")\n",
    "    video_path = compressed_video\n",
    "    use_webcam = False\n",
    "\n",
    "elif choice == 'w':\n",
    "    print(\"\\nðŸŽ¥ Using webcam.\")\n",
    "    video_path = 0\n",
    "    use_webcam = True\n",
    "    output_filename = None\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid choice. Please enter 'v' or 'w'.\")\n",
    "\n",
    "# --- 5. MAIN PROCESSING LOOP ---\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(f\"Cannot open video source: {video_path}\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if not use_webcam else 30\n",
    "\n",
    "out = None\n",
    "if not use_webcam:\n",
    "    out = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                          fps, (frame_width, frame_height))\n",
    "\n",
    "# A deque is a highly efficient list for adding/removing from the ends.\n",
    "# It will store the last SEQUENCE_LENGTH frames of landmark data.\n",
    "sequence_data = deque(maxlen=SEQUENCE_LENGTH)\n",
    "current_status = \"Waiting...\"\n",
    "prediction_confidence = 0.0\n",
    "\n",
    "with mp_pose.Pose(\n",
    "    static_image_mode=False, model_complexity=1,\n",
    "    min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    ") as pose:\n",
    "\n",
    "    # Determine total frames for tqdm progress bar\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if not use_webcam else None\n",
    "    \n",
    "    # Initialize tqdm\n",
    "    progress_bar = tqdm(total=total_frames, desc=\"Processing Video\", unit=\"frame\") if not use_webcam else None\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if use_webcam:\n",
    "            frame = cv2.flip(frame, 1) # Mirror webcam feed\n",
    "            \n",
    "        # --- Landmark Extraction ---\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "        \n",
    "        # --- Feature Vector Creation ---\n",
    "        frame_features = []\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            body_landmark_indices = list(range(11, 33)) # 22 body landmarks\n",
    "\n",
    "            for index in body_landmark_indices:\n",
    "                lm = landmarks[index]\n",
    "                frame_features.extend([lm.x, lm.y, lm.z])\n",
    "        else:\n",
    "            # If no landmarks, append a zero vector\n",
    "            frame_features = [0.0] * NUM_FEATURES\n",
    "            \n",
    "        sequence_data.append(frame_features)\n",
    "\n",
    "        # --- Prediction Logic ---\n",
    "        # Only predict when we have a full sequence\n",
    "        if len(sequence_data) == SEQUENCE_LENGTH:\n",
    "            input_data = np.expand_dims(np.array(sequence_data), axis=0)\n",
    "            prediction = model.predict(input_data, verbose=0)[0][0]\n",
    "            prediction_confidence = prediction\n",
    "            \n",
    "            current_status = \"Correct\" if prediction > 0.5 else \"Wrong\"\n",
    "\n",
    "        # --- Annotation and Display ---\n",
    "        annotated_image = frame.copy()\n",
    "        if results.pose_landmarks:\n",
    "            annotated_image = draw_body_landmarks(annotated_image, results.pose_landmarks)\n",
    "        \n",
    "        annotated_image = draw_prediction_info(annotated_image, current_status, prediction_confidence)\n",
    "\n",
    "        if use_webcam:\n",
    "            cv2.imshow(\"Push-Up Form Checker\", annotated_image)\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            out.write(annotated_image)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "# --- 6. CLEANUP ---\n",
    "if progress_bar:\n",
    "    progress_bar.close()\n",
    "cap.release()\n",
    "if out:\n",
    "    out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if not use_webcam:\n",
    "    print(f\"\\nâœ… Processing complete. Output saved as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f38fd5",
   "metadata": {},
   "source": [
    "OVERFITTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a14f39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading TensorFlow Model ---\n",
      "âœ… Model loaded successfully.\n",
      "\n",
      "--- Step 1: Compressing 'D:\\visionpipeline\\sampledata\\example2.mp4' ---\n",
      "Video is below 720p. Removing audio only...\n",
      "âœ… Compression complete. Processing 'compressed_videos\\compressed_example2.mp4'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 325/325 [00:26<00:00, 12.41frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Processing complete. Output saved to 'output_videos\\output_example2.mp4'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. SETUP, CONSTANTS & MODEL LOADING ---\n",
    "\n",
    "# --- Model and Processing Constants ---\n",
    "MODEL_PATH = 'best_model.h5'  # <--- IMPORTANT: SET YOUR MODEL FILENAME HERE\n",
    "SEQUENCE_LENGTH = 150\n",
    "NUM_FEATURES = 66\n",
    "\n",
    "# --- Directory Setup ---\n",
    "COMPRESSED_FOLDER = \"compressed_videos\"\n",
    "OUTPUT_FOLDER = \"output_videos\"\n",
    "os.makedirs(COMPRESSED_FOLDER, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# --- Load the Trained GRU Model ---\n",
    "print(\"--- Loading TensorFlow Model ---\")\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "print(\"âœ… Model loaded successfully.\")\n",
    "\n",
    "# --- MediaPipe Initialization ---\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# --- 2. HELPER FUNCTIONS ---\n",
    "\n",
    "def draw_body_landmarks(image, landmarks):\n",
    "    \"\"\"Draws the body skeleton on an image.\"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    # Define connections between keypoints to form a skeleton\n",
    "    pairs = [\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n",
    "        (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n",
    "        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),\n",
    "        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST),\n",
    "        (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE),\n",
    "        (mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE),\n",
    "        (mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE),\n",
    "        (mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE),\n",
    "    ]\n",
    "    # Draw lines for each pair of connected keypoints\n",
    "    for a, b in pairs:\n",
    "        pa, pb = landmarks.landmark[a.value], landmarks.landmark[b.value]\n",
    "        if pa.visibility > 0.5 and pb.visibility > 0.5:\n",
    "            cv2.line(image, (int(pa.x * w), int(pa.y * h)), (int(pb.x * w), int(pb.y * h)), (255, 0, 0), 2)\n",
    "            cv2.circle(image, (int(pa.x * w), int(pa.y * h)), 4, (0, 0, 255), -1)\n",
    "            cv2.circle(image, (int(pb.x * w), int(pb.y * h)), 4, (0, 0, 255), -1)\n",
    "    return image\n",
    "\n",
    "def draw_prediction_info(image, status, confidence):\n",
    "    \"\"\"Draws the prediction status and confidence on the image.\"\"\"\n",
    "    color = (0, 255, 0) if status == \"Correct\" else (0, 0, 255)\n",
    "    cv2.rectangle(image, (0, 0), (320, 60), (245, 117, 16), -1)\n",
    "    cv2.putText(image, 'STATUS', (15, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, status, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "    cv2.putText(image, 'CONF', (220, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, f'{confidence:.2%}', (210, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_video_resolution(video_path):\n",
    "    \"\"\"Gets video resolution using ffprobe.\"\"\"\n",
    "    command = ['ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
    "               '-show_entries', 'stream=width,height', '-of', 'json', video_path]\n",
    "    try:\n",
    "        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        info = json.loads(result.stdout)\n",
    "        return info['streams'][0]['width'], info['streams'][0]['height']\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"Error: ffprobe not found. Please ensure ffmpeg is installed and in your system's PATH.\")\n",
    "        return None, None\n",
    "\n",
    "# --- 3. USER INPUT ---\n",
    "\n",
    "choice = input(\"Enter 'v' for video upload or 'w' for webcam: \").lower().strip()\n",
    "\n",
    "# --- 4. VIDEO vs WEBCAM LOGIC --- (THIS BLOCK IS UPDATED)\n",
    "\n",
    "if choice == 'v':\n",
    "    input_video = input(\"Enter the full path to your video file: \").strip().strip('\"')\n",
    "    if not os.path.exists(input_video):\n",
    "        raise FileNotFoundError(f\"The file was not found at: {input_video}\")\n",
    "\n",
    "    # Generate organized file paths for compressed and final output\n",
    "    base_filename = os.path.basename(input_video)\n",
    "    compressed_video = os.path.join(COMPRESSED_FOLDER, f\"compressed_{base_filename}\")\n",
    "    output_filename = os.path.join(OUTPUT_FOLDER, f\"output_{base_filename}\")\n",
    "\n",
    "    print(f\"\\n--- Step 1: Compressing '{input_video}' ---\")\n",
    "\n",
    "    width, height = get_video_resolution(input_video)\n",
    "    if width is None:\n",
    "        raise IOError(\"Could not read video resolution. Is ffprobe installed?\")\n",
    "\n",
    "    # This is your proven FFmpeg logic\n",
    "    if height < 720:\n",
    "        print(\"Video is below 720p. Removing audio only...\")\n",
    "        command = ['ffmpeg', '-i', input_video, '-c', 'copy', '-an', compressed_video, '-y']\n",
    "    else:\n",
    "        print(\"Video is 720p or higher. Resizing to 720p, removing audio, and compressing...\")\n",
    "        command = [\n",
    "            'ffmpeg', '-i', input_video,\n",
    "            '-vf', 'scale=trunc((iw/ih)*720/2)*2:720,setsar=1:1',\n",
    "            '-an', '-vcodec', 'libx264', '-crf', '28', compressed_video, '-y'\n",
    "        ]\n",
    "\n",
    "    # Run the command, hiding the verbose output unless there's an error\n",
    "    try:\n",
    "        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"\\n--- FFMPEG ERROR ---\")\n",
    "        print(\"FFmpeg command failed. Error message:\")\n",
    "        print(e.stderr.decode())\n",
    "        print(\"--------------------\")\n",
    "        raise\n",
    "\n",
    "    print(f\"âœ… Compression complete. Processing '{compressed_video}'...\")\n",
    "    video_path = compressed_video\n",
    "    use_webcam = False\n",
    "\n",
    "elif choice == 'w':\n",
    "    print(\"\\nðŸŽ¥ Using webcam.\")\n",
    "    video_path = 0\n",
    "    use_webcam = True\n",
    "    output_filename = None\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid choice. Please enter 'v' or 'w'.\")\n",
    "\n",
    "# --- 5. MAIN PROCESSING LOOP ---\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(f\"Cannot open video source: {video_path}\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if not use_webcam else 30\n",
    "\n",
    "out = None\n",
    "if not use_webcam:\n",
    "    out = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                          fps, (frame_width, frame_height))\n",
    "\n",
    "sequence_data = deque(maxlen=SEQUENCE_LENGTH)\n",
    "current_status = \"Waiting...\"\n",
    "prediction_confidence = 0.0\n",
    "\n",
    "with mp_pose.Pose(\n",
    "    static_image_mode=False, model_complexity=1,\n",
    "    min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    ") as pose:\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if not use_webcam else None\n",
    "    progress_bar = tqdm(total=total_frames, desc=\"Analyzing Video\", unit=\"frame\") if not use_webcam else None\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if use_webcam:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "        \n",
    "        frame_features = []\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            body_landmark_indices = list(range(11, 33))\n",
    "            for index in body_landmark_indices:\n",
    "                lm = landmarks[index]\n",
    "                frame_features.extend([lm.x, lm.y, lm.z])\n",
    "        else:\n",
    "            frame_features = [0.0] * NUM_FEATURES\n",
    "            \n",
    "        sequence_data.append(frame_features)\n",
    "\n",
    "        if len(sequence_data) == SEQUENCE_LENGTH:\n",
    "            input_data = np.expand_dims(np.array(sequence_data), axis=0)\n",
    "            prediction = model.predict(input_data, verbose=0)[0][0]\n",
    "            prediction_confidence = prediction\n",
    "            current_status = \"Correct\" if prediction > 0.5 else \"Wrong\"\n",
    "\n",
    "        annotated_image = frame.copy()\n",
    "        if results.pose_landmarks:\n",
    "            annotated_image = draw_body_landmarks(annotated_image, results.pose_landmarks)\n",
    "        \n",
    "        annotated_image = draw_prediction_info(annotated_image, current_status, prediction_confidence)\n",
    "\n",
    "        if use_webcam:\n",
    "            cv2.imshow(\"Push-Up Form Checker\", annotated_image)\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            out.write(annotated_image)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "# --- 6. CLEANUP ---\n",
    "if progress_bar:\n",
    "    progress_bar.close()\n",
    "cap.release()\n",
    "if out:\n",
    "    out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if not use_webcam:\n",
    "    print(f\"\\nâœ… Processing complete. Output saved to '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ef02e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 8.0-essentials_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with gcc 15.2.0 (Rev8, Built by MSYS2 project)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-zlib --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-sdl2 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-openal --enable-libgme --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libtheora --enable-libvo-amrwbenc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-librubberband\n",
      "  libavutil      60.  8.100 / 60.  8.100\n",
      "  libavcodec     62. 11.100 / 62. 11.100\n",
      "  libavformat    62.  3.100 / 62.  3.100\n",
      "  libavdevice    62.  1.100 / 62.  1.100\n",
      "  libavfilter    11.  4.100 / 11.  4.100\n",
      "  libswscale      9.  1.100 /  9.  1.100\n",
      "  libswresample   6.  1.100 /  6.  1.100\n",
      "Unrecognized option '-version'.\n",
      "Error splitting the argument list: Option not found\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg --version\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionpipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
