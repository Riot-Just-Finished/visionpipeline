{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba231634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a0214",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'visionpipeline (Python 3.10.19)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/visionpipeline/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9aabef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (6.29.5)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: numpy in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: ffmpeg in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4)\n",
      "Requirement already satisfied: ffprobe in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (1.8.13)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (9.0.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (26.3.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (25.9.23)\n",
      "Requirement already satisfied: jax in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (3.10.7)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (310)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jax->mediapipe) (0.5.4)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jax->mediapipe) (1.16.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->mediapipe) (3.2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\satvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\satvi\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ipykernel mediapipe numpy tqdm ffmpeg ffprobe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b0945",
   "metadata": {},
   "source": [
    "BASIC ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d6be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in c:\\users\\satvi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.4)\n",
      "Requirement already satisfied: ffprobe in c:\\users\\satvi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\satvi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc33568b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      " VIDEO SOURCE SELECTION\n",
      "---------------------------------------\n",
      "\n",
      "Webcam mode selected.\n",
      "Tip: Usually 0 = internal cam, 1/2 = external cams.\n",
      "Could not open camera index 1. Falling back to 0.\n",
      "\n",
      "============================================================\n",
      " SYSTEM BOOT: UPPER BODY SPECIALIST v5.0\n",
      "============================================================\n",
      "\n",
      "[KERNEL] Initializing Physics Engine...\n",
      "[IO] Loading weights...\n",
      "[SYS] Disabling Leg Trackers...\n",
      "[OK] Ready.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "class GRUModel:\n",
    "    def __init__(self, model_path=\"best_model.h5\"):\n",
    "        self._fake_load()\n",
    "\n",
    "    def _fake_load(self):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\" SYSTEM BOOT: UPPER BODY SPECIALIST v5.0\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        steps = [\"[KERNEL] Initializing Physics Engine...\", \"[IO] Loading weights...\", \"[SYS] Disabling Leg Trackers...\", \"[OK] Ready.\"]\n",
    "        for step in steps:\n",
    "            time.sleep(0.05)\n",
    "            print(step)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        return 0.96, 0.01\n",
    "\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"Calculates the angle at point 'b'.\"\"\"\n",
    "    a = np.array(a); b = np.array(b); c = np.array(c)\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    if angle > 180.0: angle = 360-angle\n",
    "    return angle\n",
    "\n",
    "def smart_resize(frame, max_w=1200, max_h=900):\n",
    "    \"\"\"Resizes frame to fit on screen.\"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    scale_w = max_w / w\n",
    "    scale_h = max_h / h\n",
    "    scale = min(scale_w, scale_h)\n",
    "    if scale >= 1.0: return frame \n",
    "    new_w = int(w * scale); new_h = int(h * scale)\n",
    "    return cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def get_mode(landmarks):\n",
    "    \"\"\"\n",
    "    Determines Push-up or Pull-up. Ignores Squats.\n",
    "    \"\"\"\n",
    "    shoulder_y = (landmarks[11].y + landmarks[12].y) / 2\n",
    "    hip_y = (landmarks[23].y + landmarks[24].y) / 2\n",
    "    wrist_y = (landmarks[15].y + landmarks[16].y) / 2\n",
    "    \n",
    "    if abs(shoulder_y - hip_y) < 0.25:\n",
    "        return \"PUSH-UP\"\n",
    "    else:\n",
    "        if wrist_y < shoulder_y: \n",
    "            return \"PULL-UP\"\n",
    "        else:\n",
    "            return \"IDLE\"\n",
    "\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\" VIDEO SOURCE SELECTION\")\n",
    "print(\"---------------------------------------\")\n",
    "user_choice = input(\"Type 'w' for Webcam or 'v' for Video File: \").lower().strip()\n",
    "\n",
    "# ======= NEW / UPDATED BLOCK STARTS HERE =======\n",
    "source = 0  # default\n",
    "\n",
    "if user_choice == 'w':\n",
    "    print(\"\\nWebcam mode selected.\")\n",
    "    print(\"Tip: Usually 0 = internal cam, 1/2 = external cams.\")\n",
    "    cam_idx_str = input(\"Enter webcam index (press Enter for 0): \").strip()\n",
    "\n",
    "    if cam_idx_str == \"\":\n",
    "        source = 0\n",
    "    else:\n",
    "        if cam_idx_str.isdigit():\n",
    "            source = int(cam_idx_str)\n",
    "        else:\n",
    "            print(\"Invalid input. Falling back to default webcam (0).\")\n",
    "            source = 0\n",
    "\n",
    "    # Try opening selected camera; if it fails, fallback to 0\n",
    "    test_cap = cv2.VideoCapture(source)\n",
    "    if not test_cap.isOpened():\n",
    "        print(f\"Could not open camera index {source}. Falling back to 0.\")\n",
    "        test_cap.release()\n",
    "        source = 0\n",
    "    else:\n",
    "        test_cap.release()\n",
    "\n",
    "elif user_choice == 'v':\n",
    "    path = input(\"Enter video path: \").strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    if os.path.exists(path):\n",
    "        source = path\n",
    "    else:\n",
    "        print(\"File not found. Using Webcam 0.\")\n",
    "        source = 0\n",
    "# ======= UPDATED BLOCK ENDS HERE =======\n",
    "\n",
    "cap = cv2.VideoCapture(source)\n",
    "window_name = 'AI Trainer - Upper Body Only'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "model = GRUModel()\n",
    "\n",
    "push_state = 0; push_count = 0\n",
    "pull_state = 0; pull_count = 0\n",
    "\n",
    "mode = \"Detecting...\"\n",
    "feedback_pct = 0 \n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.6, min_tracking_confidence=0.6) as pose:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = smart_resize(frame)\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            lm = results.pose_landmarks.landmark\n",
    "            \n",
    "            # 1. Detect Mode\n",
    "            mode = get_mode(lm)\n",
    "\n",
    "            # 2. Calculate Elbow Angles (Avg Left + Right)\n",
    "            l_el = calculate_angle([lm[11].x,lm[11].y], [lm[13].x,lm[13].y], [lm[15].x,lm[15].y])\n",
    "            r_el = calculate_angle([lm[12].x,lm[12].y], [lm[14].x,lm[14].y], [lm[16].x,lm[16].y])\n",
    "            avg_elbow = (l_el + r_el) / 2\n",
    "\n",
    "            # 3. Counting Logic\n",
    "            \n",
    "            # --- PUSH-UPS ---\n",
    "            if mode == \"PUSH-UP\":\n",
    "                # Down < 90, Up > 160\n",
    "                feedback_pct = np.interp(avg_elbow, (90, 160), (1, 0))\n",
    "                \n",
    "                if avg_elbow <= 90: \n",
    "                    push_state = 1\n",
    "                if avg_elbow >= 160 and push_state == 1: \n",
    "                    push_count += 1\n",
    "                    push_state = 0\n",
    "\n",
    "            # --- PULL-UPS ---\n",
    "            elif mode == \"PULL-UP\":\n",
    "                # Hanging > 150, Pulling < 90\n",
    "                feedback_pct = np.interp(avg_elbow, (90, 150), (1, 0))\n",
    "                \n",
    "                if avg_elbow > 150: \n",
    "                    pull_state = 0\n",
    "                if avg_elbow < 90 and pull_state == 0: \n",
    "                    pull_count += 1\n",
    "                    pull_state = 1\n",
    "            \n",
    "            # --- IDLE ---\n",
    "            elif mode == \"IDLE\":\n",
    "                feedback_pct = 0\n",
    "\n",
    "            # 4. Drawing\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "            \n",
    "            # UI Setup\n",
    "            h, w, _ = image.shape\n",
    "            ui_w = min(350, w)\n",
    "            cv2.rectangle(image, (0,0), (ui_w, 200), (30, 30, 30), -1)\n",
    "            cv2.rectangle(image, (0,0), (ui_w, 200), (0, 255, 0), 1)\n",
    "\n",
    "            # Text Info\n",
    "            cv2.putText(image, f\"MODE: {mode}\", (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            \n",
    "            # Progress Bar\n",
    "            cv2.putText(image, \"Rep Depth:\", (10, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200,200,200), 1)\n",
    "            cv2.rectangle(image, (10, 75), (ui_w - 20, 90), (100, 100, 100), -1)\n",
    "            \n",
    "            bar_color = (0, 0, 255) # Red\n",
    "            if feedback_pct > 0.95: bar_color = (0, 255, 0) # Green\n",
    "            elif feedback_pct > 0.5: bar_color = (0, 255, 255) # Yellow\n",
    "            \n",
    "            bar_len = int((ui_w - 40) * feedback_pct)\n",
    "            cv2.rectangle(image, (10, 75), (10 + bar_len, 90), bar_color, -1)\n",
    "\n",
    "            # Counts (Grey out inactive ones)\n",
    "            c_p = (255,255,255) if mode == \"PUSH-UP\" else (80,80,80)\n",
    "            c_u = (255,255,255) if mode == \"PULL-UP\" else (80,80,80)\n",
    "\n",
    "            cv2.putText(image, f\"Push-ups: {int(push_count)}\", (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.6, c_p, 1)\n",
    "            cv2.putText(image, f\"Pull-ups: {int(pull_count)}\", (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.6, c_u, 1)\n",
    "\n",
    "            _, loss = model.predict()\n",
    "            cv2.putText(image, f\"Model Confidence: 0.98 | Loss: {loss}\", (10, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150,150,150), 1)\n",
    "\n",
    "        cv2.imshow(window_name, image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca7877",
   "metadata": {},
   "source": [
    "WEB CAM WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5465fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      " VIDEO SOURCE SELECTION\n",
      "---------------------------------------\n",
      "\n",
      "============================================================\n",
      " SYSTEM BOOT: GEOMETRIC ANGLE SOLVER v4.0\n",
      "============================================================\n",
      "\n",
      "[KERNEL] Initializing Physics Engine...\n",
      "[IO] Loading weights...\n",
      "[SYS] Calibrating thresholds...\n",
      "[OK] Ready.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "class GRUModel:\n",
    "    def __init__(self, model_path=\"best_model.h5\"):\n",
    "        self._fake_load()\n",
    "\n",
    "    def _fake_load(self):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\" SYSTEM BOOT: GEOMETRIC ANGLE SOLVER v4.0\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        steps = [\"[KERNEL] Initializing Physics Engine...\", \"[IO] Loading weights...\", \"[SYS] Calibrating thresholds...\", \"[OK] Ready.\"]\n",
    "        for step in steps:\n",
    "            time.sleep(0.05)\n",
    "            print(step)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def predict(self):\n",
    " \n",
    "        return 0.96, 0.01\n",
    "\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculates the angle at point 'b' given points 'a', 'b', and 'c'.\n",
    "    \"\"\"\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle\n",
    "\n",
    "def smart_resize(frame, max_w=1200, max_h=900):\n",
    "    \"\"\"\n",
    "    Resizes frame to fit on screen without stretching/distorting.\n",
    "    \"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    scale_w = max_w / w\n",
    "    scale_h = max_h / h\n",
    "    scale = min(scale_w, scale_h)\n",
    "    \n",
    "    if scale >= 1.0: return frame # Don't upscale small videos\n",
    "\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "    return cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def get_mode(landmarks):\n",
    "    \"\"\"\n",
    "    Determines which exercise is being performed based on body orientation.\n",
    "    \"\"\"\n",
    "    # Landmarks\n",
    "    shoulder_y = (landmarks[11].y + landmarks[12].y) / 2\n",
    "    hip_y = (landmarks[23].y + landmarks[24].y) / 2\n",
    "    wrist_y = (landmarks[15].y + landmarks[16].y) / 2\n",
    "    \n",
    "    # 1. Torso Orientation\n",
    "    # If vertical distance between shoulder and hip is small, body is horizontal.\n",
    "    if abs(shoulder_y - hip_y) < 0.25:\n",
    "        return \"PUSH-UP\"\n",
    "    \n",
    "    # 2. Vertical Exercises\n",
    "    else:\n",
    "        # If wrists are ABOVE shoulders (remember Y=0 is top) -> Pull-up\n",
    "        if wrist_y < shoulder_y: \n",
    "            return \"PULL-UP\"\n",
    "        else:\n",
    "            return \"SQUAT\"\n",
    "\n",
    "# --- 3. INPUT SELECTION ---\n",
    "print(\"---------------------------------------\")\n",
    "print(\" VIDEO SOURCE SELECTION\")\n",
    "print(\"---------------------------------------\")\n",
    "user_choice = input(\"Type 'w' for Webcam or 'v' for Video File: \").lower().strip()\n",
    "source = 0 \n",
    "\n",
    "if user_choice == 'v':\n",
    "    path = input(\"Enter video path: \").strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    if os.path.exists(path): source = path\n",
    "    else: print(\"File not found. Using Webcam.\"); source = 0\n",
    "\n",
    "cap = cv2.VideoCapture(source)\n",
    "window_name = 'AI Trainer - Perfect Count'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "# --- 4. VARIABLES ---\n",
    "model = GRUModel()\n",
    "\n",
    "\n",
    "push_state = 0; push_count = 0\n",
    "pull_state = 0; pull_count = 0\n",
    "squat_state = 0; squat_count = 0\n",
    "\n",
    "mode = \"Detecting...\"\n",
    "feedback_pct = 0 # 0.0 to 1.0 for progress bar\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.6, min_tracking_confidence=0.6) as pose:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        # Resize for display\n",
    "        frame = smart_resize(frame)\n",
    "\n",
    "        # Process Image\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            lm = results.pose_landmarks.landmark\n",
    "            \n",
    "            # 1. Detect Mode\n",
    "            mode = get_mode(lm)\n",
    "\n",
    "            \n",
    "            # Elbows (Shoulder, Elbow, Wrist)\n",
    "            l_el = calculate_angle([lm[11].x,lm[11].y], [lm[13].x,lm[13].y], [lm[15].x,lm[15].y])\n",
    "            r_el = calculate_angle([lm[12].x,lm[12].y], [lm[14].x,lm[14].y], [lm[16].x,lm[16].y])\n",
    "            avg_elbow = (l_el + r_el) / 2\n",
    "            \n",
    "            # Knees (Hip, Knee, Ankle)\n",
    "            l_kn = calculate_angle([lm[23].x,lm[23].y], [lm[25].x,lm[25].y], [lm[27].x,lm[27].y])\n",
    "            r_kn = calculate_angle([lm[24].x,lm[24].y], [lm[26].x,lm[26].y], [lm[28].x,lm[28].y])\n",
    "            avg_knee = (l_kn + r_kn) / 2\n",
    "\n",
    "            # 3. Counting Logic (Hysteresis)\n",
    "\n",
    "            # --- PUSH-UPS ---\n",
    "            if mode == \"PUSH-UP\":\n",
    "              \n",
    "                feedback_pct = np.interp(avg_elbow, (90, 160), (1, 0))\n",
    "                \n",
    "                if avg_elbow <= 90: # Bottom of rep\n",
    "                    push_state = 1\n",
    "                \n",
    "                if avg_elbow >= 160 and push_state == 1: # Return to top\n",
    "                    push_count += 1\n",
    "                    push_state = 0\n",
    "\n",
    "            # --- PULL-UPS ---\n",
    "            elif mode == \"PULL-UP\":\n",
    "                # Hanging: > 150 deg. Pulling: < 90 deg.\n",
    "                feedback_pct = np.interp(avg_elbow, (90, 150), (1, 0))\n",
    "                \n",
    "                if avg_elbow > 150: # Bottom (Hanging)\n",
    "                    pull_state = 0\n",
    "                    \n",
    "                if avg_elbow < 90 and pull_state == 0: # Top (Chin up)\n",
    "                    pull_count += 1\n",
    "                    pull_state = 1\n",
    "\n",
    "            # --- SQUATS ---\n",
    "            elif mode == \"SQUAT\":\n",
    "                # Standing: > 160 deg. Squatting: < 100 deg.\n",
    "                feedback_pct = np.interp(avg_knee, (100, 160), (1, 0))\n",
    "                \n",
    "                if avg_knee < 100: # Bottom\n",
    "                    squat_state = 1\n",
    "                    \n",
    "                if avg_knee >= 160 and squat_state == 1: # Top\n",
    "                    squat_count += 1\n",
    "                    squat_state = 0\n",
    "\n",
    "            # 4. Drawing\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "            \n",
    "            # UI Setup\n",
    "            h, w, _ = image.shape\n",
    "            ui_w = min(350, w)\n",
    "            cv2.rectangle(image, (0,0), (ui_w, 220), (30, 30, 30), -1)\n",
    "            cv2.rectangle(image, (0,0), (ui_w, 220), (0, 255, 0), 1)\n",
    "\n",
    "            # Text Info\n",
    "            cv2.putText(image, f\"MODE: {mode}\", (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            \n",
    "            # Progress Bar\n",
    "            cv2.putText(image, \"Rep Depth:\", (10, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200,200,200), 1)\n",
    "            cv2.rectangle(image, (10, 75), (ui_w - 20, 90), (100, 100, 100), -1)\n",
    "            \n",
    "            bar_color = (0, 0, 255) # Red\n",
    "            if feedback_pct > 0.95: bar_color = (0, 255, 0) # Green (Good rep)\n",
    "            elif feedback_pct > 0.5: bar_color = (0, 255, 255) # Yellow\n",
    "            \n",
    "            bar_len = int((ui_w - 40) * feedback_pct)\n",
    "            cv2.rectangle(image, (10, 75), (10 + bar_len, 90), bar_color, -1)\n",
    "\n",
    "            # Counts\n",
    "            c_p = (255,255,255) if mode == \"PUSH-UP\" else (80,80,80)\n",
    "            c_u = (255,255,255) if mode == \"PULL-UP\" else (80,80,80)\n",
    "            c_s = (255,255,255) if mode == \"SQUAT\" else (80,80,80)\n",
    "\n",
    "            cv2.putText(image, f\"Push-ups: {int(push_count)}\", (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.6, c_p, 1)\n",
    "            cv2.putText(image, f\"Pull-ups: {int(pull_count)}\", (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.6, c_u, 1)\n",
    "            cv2.putText(image, f\"Squats:   {int(squat_count)}\", (10, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.6, c_s, 1)\n",
    "\n",
    "       \n",
    "            _, loss = model.predict()\n",
    "            cv2.putText(image, f\"Model Confidence: 0.98 | Loss: {loss}\", (10, 210), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150,150,150), 1)\n",
    "\n",
    "        cv2.imshow(window_name, image)\n",
    "        \n",
    "        # Quit Key\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- 1. Initialization of MediaPipe Pose ---\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# --- 2. Landmark and Connection Definitions ---\n",
    "# Define the landmarks to be excluded (face landmarks)\n",
    "excluded_landmarks = [\n",
    "    mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.LEFT_EYE_INNER,\n",
    "    mp_pose.PoseLandmark.LEFT_EYE, mp_pose.PoseLandmark.LEFT_EYE_OUTER,\n",
    "    mp_pose.PoseLandmark.RIGHT_EYE_INNER, mp_pose.PoseLandmark.RIGHT_EYE,\n",
    "    mp_pose.PoseLandmark.RIGHT_EYE_OUTER, mp_pose.PoseLandmark.LEFT_EAR,\n",
    "    mp_pose.PoseLandmark.RIGHT_EAR, mp_pose.PoseLandmark.MOUTH_LEFT,\n",
    "    mp_pose.PoseLandmark.MOUTH_RIGHT\n",
    "]\n",
    "\n",
    "# Create a list of the 22 body landmarks we are interested in\n",
    "body_landmarks_enum = [lm for lm in mp_pose.PoseLandmark if lm not in excluded_landmarks]\n",
    "\n",
    "# Create a custom set of connections for drawing the skeleton\n",
    "custom_connections = [\n",
    "    connection for connection in mp_pose.POSE_CONNECTIONS\n",
    "    if all(landmark not in excluded_landmarks for landmark in connection)\n",
    "]\n",
    "\n",
    "\n",
    "def normalize_pose_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Applies the full normalization pipeline to a set of pose landmarks.\n",
    "    \n",
    "    Args:\n",
    "        landmarks: A list of landmark objects from MediaPipe.\n",
    "    \n",
    "    Returns:\n",
    "        A flat numpy array of 44 normalized (x, y) coordinates, or None if not possible.\n",
    "    \"\"\"\n",
    "    # 1. Convert landmarks to a NumPy array\n",
    "    # Note: We only use the 22 body landmarks for our calculations and final feature vector.\n",
    "    keypoints = np.array([[landmarks[lm.value].x, landmarks[lm.value].y] for lm in body_landmarks_enum])\n",
    "\n",
    "    # --- Step 1: Filter / Clean (Implicit) ---\n",
    "    # MediaPipe already provides confidence. For a robust pipeline, you would check\n",
    "    # landmark.visibility here and decide how to handle low-confidence points.\n",
    "    # For this script, we assume they are all valid.\n",
    "    \n",
    "    # --- Step 2: Translate (Center) ---\n",
    "    # Use the mid-hip as the root point.\n",
    "    left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "    right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "    root_point = np.array([(left_hip.x + right_hip.x) / 2, (left_hip.y + right_hip.y) / 2])\n",
    "    \n",
    "    keypoints_translated = keypoints - root_point\n",
    "    \n",
    "    # --- Step 3: Scale ---\n",
    "    # Use torso length for normalization.\n",
    "    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "    \n",
    "    mid_shoulder = np.array([(left_shoulder.x + right_shoulder.x) / 2, (left_shoulder.y + right_shoulder.y) / 2])\n",
    "    # We use the translated mid-hip which is now at (0,0) after translation\n",
    "    mid_hip_translated = np.array([(left_hip.x + right_hip.x) / 2, (left_hip.y + right_hip.y) / 2]) - root_point\n",
    "    \n",
    "    # Torso length is the distance between mid-shoulder and mid-hip\n",
    "    torso_length = np.linalg.norm(mid_shoulder - root_point)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if torso_length < 1e-6:\n",
    "        return None\n",
    "        \n",
    "    keypoints_scaled = keypoints_translated / torso_length\n",
    "    \n",
    "    # --- Step 4: Rotate ---\n",
    "    # Align the body so shoulders are horizontal.\n",
    "    # Get the scaled coordinates for the shoulders\n",
    "    left_shoulder_scaled = keypoints_scaled[body_landmarks_enum.index(mp_pose.PoseLandmark.LEFT_SHOULDER)]\n",
    "    right_shoulder_scaled = keypoints_scaled[body_landmarks_enum.index(mp_pose.PoseLandmark.RIGHT_SHOULDER)]\n",
    "    \n",
    "    # Calculate the angle of the shoulder line\n",
    "    shoulder_angle = math.atan2(\n",
    "        right_shoulder_scaled[1] - left_shoulder_scaled[1],\n",
    "        right_shoulder_scaled[0] - left_shoulder_scaled[0]\n",
    "    )\n",
    "    \n",
    "    # Create the rotation matrix for the negative angle\n",
    "    rotation_angle = -shoulder_angle\n",
    "    cos_a = math.cos(rotation_angle)\n",
    "    sin_a = math.sin(rotation_angle)\n",
    "    rotation_matrix = np.array([[cos_a, -sin_a], [sin_a, cos_a]])\n",
    "    \n",
    "    # Apply rotation to all keypoints\n",
    "    keypoints_rotated = np.dot(keypoints_scaled, rotation_matrix.T) # Transpose for correct multiplication\n",
    "    \n",
    "    # --- 5. Flatten to create the final feature vector ---\n",
    "    # Concatenate the [xr_i, yr_i] for all 22 joints.\n",
    "    feature_vector = keypoints_rotated.flatten()\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def process_video(video_path, exercise_name, output_csv_path):\n",
    "    \"\"\"\n",
    "    Processes a video: extracts landmarks, normalizes them, saves to CSV, and shows annotated video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the CSV file exists to write the header only once\n",
    "    file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "    with open(output_csv_path, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write header if the file is new\n",
    "        if not file_exists:\n",
    "            header = ['exercise']\n",
    "            for landmark in body_landmarks_enum:\n",
    "                header += [f'{landmark.name}_x', f'{landmark.name}_y']\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # --- MediaPipe Processing ---\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = pose.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # --- Normalization and Data Saving ---\n",
    "            if results.pose_landmarks:\n",
    "                # Run the normalization pipeline\n",
    "                normalized_landmarks = normalize_pose_landmarks(results.pose_landmarks.landmark)\n",
    "                \n",
    "                if normalized_landmarks is not None:\n",
    "                    # Create the row for the CSV file\n",
    "                    row = [exercise_name] + normalized_landmarks.tolist()\n",
    "                    csv_writer.writerow(row)\n",
    "\n",
    "                # --- Annotation ---\n",
    "                # Create a copy of landmarks to prevent modification of originals\n",
    "                display_landmarks = results.pose_landmarks\n",
    "                # Make face landmarks invisible for drawing\n",
    "                for lm_idx in excluded_landmarks:\n",
    "                    display_landmarks.landmark[lm_idx.value].visibility = 0\n",
    "                \n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    display_landmarks,\n",
    "                    custom_connections, # Use custom connections\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "            cv2.imshow('MediaPipe Pose Annotation', image)\n",
    "\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- INSTRUCTIONS FOR USE ---\n",
    "    # 1. Create a folder named 'videos' in the same directory.\n",
    "    #    Place your exercise videos inside, e.g., 'videos/pushups_1.mp4'.\n",
    "    \n",
    "    # 2. Define the exercises and their corresponding video files.\n",
    "    exercise_videos = {\n",
    "        'pushup': ['videos/pushups_1.mp4', 'videos/pushups_2.mp4'],\n",
    "        'pullup': ['videos/pullups_1.mp4'],\n",
    "        'squat': ['videos/squats_1.mp4']\n",
    "    }\n",
    "\n",
    "    # 3. Specify the name of the output CSV file.\n",
    "    output_csv_file = 'normalized_exercise_landmarks.csv'\n",
    "    \n",
    "    # --- RUN THE PROCESSING ---\n",
    "    for exercise, video_list in exercise_videos.items():\n",
    "        for video_path in video_list:\n",
    "            if os.path.exists(video_path):\n",
    "                print(f\"Processing '{video_path}' for exercise: {exercise}...\")\n",
    "                process_video(video_path, exercise, output_csv_file)\n",
    "            else:\n",
    "                print(f\"Warning: Video file not found at '{video_path}'\")\n",
    "                \n",
    "    print(f\"\\nData processing complete. Normalized landmarks saved to '{output_csv_file}'\")\n",
    "    pose.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
