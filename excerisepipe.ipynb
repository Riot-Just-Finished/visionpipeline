{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba231634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a0214",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'visionpipeline (Python 3.10.19)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/visionpipeline/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9aabef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in .\\.venv\\lib\\site-packages (7.1.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: mediapipe in .\\.venv\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: numpy in .\\.venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in .\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: ffmpeg in .\\.venv\\lib\\site-packages (1.4)\n",
      "Requirement already satisfied: ffprobe in .\\.venv\\lib\\site-packages (0.5)\n",
      "Requirement already satisfied: tornado>=6.2 in .\\.venv\\lib\\site-packages (from ipykernel) (6.5.2)\n",
      "Requirement already satisfied: packaging>=22 in .\\.venv\\lib\\site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in .\\.venv\\lib\\site-packages (from ipykernel) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in .\\.venv\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in .\\.venv\\lib\\site-packages (from ipykernel) (8.37.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in .\\.venv\\lib\\site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: psutil>=5.7 in .\\.venv\\lib\\site-packages (from ipykernel) (7.1.2)\n",
      "Requirement already satisfied: pyzmq>=25 in .\\.venv\\lib\\site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in .\\.venv\\lib\\site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in .\\.venv\\lib\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in .\\.venv\\lib\\site-packages (from ipykernel) (1.8.17)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in .\\.venv\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in .\\.venv\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: jax in .\\.venv\\lib\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: jaxlib in .\\.venv\\lib\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in .\\.venv\\lib\\site-packages (from mediapipe) (25.9.23)\n",
      "Requirement already satisfied: matplotlib in .\\.venv\\lib\\site-packages (from mediapipe) (3.10.7)\n",
      "Requirement already satisfied: opencv-contrib-python in .\\.venv\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: sentencepiece in .\\.venv\\lib\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: absl-py in .\\.venv\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in .\\.venv\\lib\\site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: attrs>=19.1.0 in .\\.venv\\lib\\site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: stack_data in .\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in .\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (4.15.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in .\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in .\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: exceptiongroup in .\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in .\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: decorator in .\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in .\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.5.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in .\\.venv\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: opt_einsum in .\\.venv\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in .\\.venv\\lib\\site-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.12 in .\\.venv\\lib\\site-packages (from jax->mediapipe) (1.15.3)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (3.2.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in .\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (12.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (4.60.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: pycparser in .\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in .\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Requirement already satisfied: wcwidth in .\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.14)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in .\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in .\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in .\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install ipykernel mediapipe numpy tqdm ffmpeg ffprobe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b0945",
   "metadata": {},
   "source": [
    "BASIC ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d6be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in c:\\users\\satvi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.4)\n",
      "Requirement already satisfied: ffprobe in c:\\users\\satvi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\satvi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc33568b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model complexity: 0\n",
      "\n",
      "--- Step 2: Compressing 'D:\\visionpipeline\\sampledata\\example2.mp4' ---\n",
      "Video is below 720p. Removing audio only...\n",
      "âœ… Compression complete. Processing 'compressed_video.mp4'...\n",
      "\n",
      "--- Step 3: Running Pose Estimation on 'compressed_video.mp4' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 325/325 [00:07<00:00, 41.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Pose estimation complete. Output saved as 'pose_output.mp4'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP & HELPER FUNCTIONS ---\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def draw_body_landmarks(image, landmarks):\n",
    "    \"\"\"Draws the body skeleton on an image.\"\"\"                                              \n",
    "    h, w, _ = image.shape\n",
    "    keypoints = [\n",
    "        mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "        mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.RIGHT_ELBOW,\n",
    "        mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.RIGHT_WRIST,\n",
    "        mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n",
    "        mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.RIGHT_KNEE,\n",
    "        mp_pose.PoseLandmark.LEFT_ANKLE, mp_pose.PoseLandmark.RIGHT_ANKLE,\n",
    "    ]\n",
    "    for lm_enum in keypoints:\n",
    "        lm = landmarks.landmark[lm_enum.value]\n",
    "        cv2.circle(image, (int(lm.x * w), int(lm.y * h)), 5, (0, 255, 0), -1)\n",
    "\n",
    "    pairs = [\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n",
    "        (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n",
    "        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),\n",
    "        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST),\n",
    "        (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE),\n",
    "        (mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE),\n",
    "        (mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE),\n",
    "        (mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE),\n",
    "    ]\n",
    "    for a, b in pairs:\n",
    "        pa, pb = landmarks.landmark[a.value], landmarks.landmark[b.value]\n",
    "        cv2.line(image, (int(pa.x * w), int(pa.y * h)), (int(pb.x * w), int(pb.y * h)), (255, 0, 0), 2)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_video_resolution(video_path):\n",
    "    \"\"\"Gets video resolution using ffprobe.\"\"\"\n",
    "    command = ['ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
    "               '-show_entries', 'stream=width,height', '-of', 'json', video_path]\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if result.returncode == 0:\n",
    "        info = json.loads(result.stdout)\n",
    "        return info['streams'][0]['width'], info['streams'][0]['height']\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# --- 2. USER INPUT ---\n",
    "\n",
    "choice = input(\"Enter 'v' for video upload or 'w' for webcam: \").lower().strip()\n",
    "\n",
    "try:\n",
    "    model_complexity_choice = int(input(\"Enter model complexity (0=light, 1=full, 2=heavy): \").strip())\n",
    "    if model_complexity_choice not in [0, 1, 2]:\n",
    "        print(\"Invalid choice. Defaulting to 1 (full).\")\n",
    "        model_complexity_choice = 1\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Defaulting to 1 (full).\")\n",
    "    model_complexity_choice = 1\n",
    "\n",
    "print(f\"Using model complexity: {model_complexity_choice}\")\n",
    "\n",
    "\n",
    "# --- 3. VIDEO vs WEBCAM LOGIC ---\n",
    "\n",
    "if choice == 'v':\n",
    "    input_video = input(\"Enter the full path to your video file: \").strip()\n",
    "    if not os.path.exists(input_video):\n",
    "        raise FileNotFoundError(f\"The file was not found at: {input_video}\")\n",
    "\n",
    "    compressed_video = \"compressed_video.mp4\"\n",
    "    print(f\"\\n--- Step 2: Compressing '{input_video}' ---\")\n",
    "\n",
    "    width, height = get_video_resolution(input_video)\n",
    "    if width is None:\n",
    "        raise IOError(\"Could not read video resolution. Is ffprobe installed?\")\n",
    "\n",
    "    if height < 720:\n",
    "        print(\"Video is below 720p. Removing audio only...\")\n",
    "        command = ['ffmpeg', '-i', input_video, '-c', 'copy', '-an', compressed_video, '-y']\n",
    "    else:\n",
    "        print(\"Video is 720p or higher. Resizing to 720p, removing audio, and compressing...\")\n",
    "        command = [\n",
    "            'ffmpeg', '-i', input_video,\n",
    "            '-vf', 'scale=trunc((iw/ih)*720/2)*2:720,setsar=1:1',\n",
    "            '-an', '-vcodec', 'libx264', '-crf', '28', compressed_video, '-y'\n",
    "        ]\n",
    "\n",
    "    subprocess.run(command, check=True)\n",
    "    print(f\"âœ… Compression complete. Processing '{compressed_video}'...\")\n",
    "    video_path = compressed_video\n",
    "    use_webcam = False\n",
    "\n",
    "elif choice == 'w':\n",
    "    print(\"\\nðŸŽ¥ Using webcam.\")\n",
    "    video_path = 0\n",
    "    use_webcam = True\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid choice. Enter 'v' or 'w'.\")\n",
    "\n",
    "\n",
    "# --- 4. POSE ESTIMATION ---\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(f\"Cannot open video source: {video_path}\")\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if not use_webcam else 30\n",
    "\n",
    "output_filename = 'pose_output.mp4'\n",
    "out = None\n",
    "if not use_webcam:\n",
    "    out = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                          fps, (frame_width, frame_height))\n",
    "\n",
    "with mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=model_complexity_choice,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as pose:\n",
    "\n",
    "    if use_webcam:\n",
    "        print(\"Press 'q' to quit webcam.\")\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image_rgb)\n",
    "\n",
    "            annotated = frame.copy()\n",
    "            if results.pose_landmarks:\n",
    "                annotated = draw_body_landmarks(annotated, results.pose_landmarks)\n",
    "\n",
    "            cv2.imshow(\"MediaPipe Pose (Webcam)\", annotated)\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f\"\\n--- Step 3: Running Pose Estimation on '{video_path}' ---\")\n",
    "        for _ in tqdm(range(frame_count)):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image_rgb)\n",
    "\n",
    "            annotated = frame.copy()\n",
    "            if results.pose_landmarks:\n",
    "                annotated = draw_body_landmarks(annotated, results.pose_landmarks)\n",
    "\n",
    "            out.write(annotated)\n",
    "\n",
    "        print(f\"\\nâœ… Pose estimation complete. Output saved as '{output_filename}'.\")\n",
    "\n",
    "# --- 5. CLEANUP ---\n",
    "cap.release()\n",
    "if out:\n",
    "    out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca7877",
   "metadata": {},
   "source": [
    "WEB CAM WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5465fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model complexity: 0\n",
      "\n",
      "ðŸŽ¥ Using webcam.\n",
      "Press 'q' to quit webcam feed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 170\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Convert the BGR image to RGB\u001b[39;00m\n\u001b[0;32m    169\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m--> 170\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Create a copy to draw on\u001b[39;00m\n\u001b[0;32m    173\u001b[0m annotated_image \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32md:\\visionpipeline\\.venv\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32md:\\visionpipeline\\.venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP & HELPER FUNCTIONS ---\n",
    "\n",
    "# Create directories for compressed and output videos if they don't exist\n",
    "COMPRESSED_FOLDER = \"compressed videos\"\n",
    "OUTPUT_FOLDER = \"output videos\"\n",
    "os.makedirs(COMPRESSED_FOLDER, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def draw_body_landmarks(image, landmarks):\n",
    "    \"\"\"Draws the body skeleton on an image.\"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    # List of keypoints to draw\n",
    "    keypoints = [\n",
    "        mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "        mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.RIGHT_ELBOW,\n",
    "        mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.RIGHT_WRIST,\n",
    "        mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n",
    "        mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.RIGHT_KNEE,\n",
    "        mp_pose.PoseLandmark.LEFT_ANKLE, mp_pose.PoseLandmark.RIGHT_ANKLE,\n",
    "    ]\n",
    "    # Draw circles for each keypoint\n",
    "    for lm_enum in keypoints:\n",
    "        lm = landmarks.landmark[lm_enum.value]\n",
    "        cv2.circle(image, (int(lm.x * w), int(lm.y * h)), 5, (0, 255, 0), -1)\n",
    "\n",
    "    # Define connections between keypoints to form a skeleton\n",
    "    pairs = [\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n",
    "        (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n",
    "        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),\n",
    "        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),\n",
    "        (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST),\n",
    "        (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE),\n",
    "        (mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE),\n",
    "        (mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE),\n",
    "        (mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE),\n",
    "    ]\n",
    "    # Draw lines for each pair of connected keypoints\n",
    "    for a, b in pairs:\n",
    "        pa, pb = landmarks.landmark[a.value], landmarks.landmark[b.value]\n",
    "        cv2.line(image, (int(pa.x * w), int(pa.y * h)), (int(pb.x * w), int(pb.y * h)), (255, 0, 0), 2)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_video_resolution(video_path):\n",
    "    \"\"\"Gets video resolution using ffprobe.\"\"\"\n",
    "    command = ['ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
    "               '-show_entries', 'stream=width,height', '-of', 'json', video_path]\n",
    "    try:\n",
    "        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        info = json.loads(result.stdout)\n",
    "        return info['streams'][0]['width'], info['streams'][0]['height']\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"Error: ffprobe not found. Please ensure ffmpeg is installed and in your system's PATH.\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# --- 2. USER INPUT ---\n",
    "\n",
    "choice = input(\"Enter 'v' for video upload or 'w' for webcam: \").lower().strip()\n",
    "\n",
    "try:\n",
    "    model_complexity_choice = int(input(\"Enter model complexity (0=light, 1=full, 2=heavy): \").strip())\n",
    "    if model_complexity_choice not in [0, 1, 2]:\n",
    "        print(\"Invalid choice. Defaulting to 1 (full).\")\n",
    "        model_complexity_choice = 1\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Defaulting to 1 (full).\")\n",
    "    model_complexity_choice = 1\n",
    "\n",
    "print(f\"Using model complexity: {model_complexity_choice}\")\n",
    "\n",
    "\n",
    "# --- 3. VIDEO vs WEBCAM LOGIC ---\n",
    "\n",
    "if choice == 'v':\n",
    "    input_video = input(\"Enter the full path to your video file: \").strip()\n",
    "    if not os.path.exists(input_video):\n",
    "        raise FileNotFoundError(f\"The file was not found at: {input_video}\")\n",
    "\n",
    "    # Generate organized file paths\n",
    "    base_filename = os.path.basename(input_video)\n",
    "    compressed_video = os.path.join(COMPRESSED_FOLDER, f\"compressed_{base_filename}\")\n",
    "    output_filename = os.path.join(OUTPUT_FOLDER, f\"output_{base_filename}\")\n",
    "\n",
    "    print(f\"\\n--- Step 2: Compressing '{input_video}' ---\")\n",
    "\n",
    "    width, height = get_video_resolution(input_video)\n",
    "    if width is None:\n",
    "        raise IOError(\"Could not read video resolution. Is ffprobe installed and accessible?\")\n",
    "\n",
    "    # Video compression logic\n",
    "    if height < 720:\n",
    "        print(\"Video is below 720p. Removing audio only...\")\n",
    "        command = ['ffmpeg', '-i', input_video, '-c', 'copy', '-an', compressed_video, '-y']\n",
    "    else:\n",
    "        print(\"Video is 720p or higher. Resizing to 720p, removing audio, and compressing...\")\n",
    "        command = [\n",
    "            'ffmpeg', '-i', input_video,\n",
    "            '-vf', 'scale=trunc((iw/ih)*720/2)*2:720,setsar=1:1',\n",
    "            '-an', '-vcodec', 'libx264', '-crf', '28', compressed_video, '-y'\n",
    "        ]\n",
    "\n",
    "    subprocess.run(command, check=True)\n",
    "    print(f\"âœ… Compression complete. Processing '{compressed_video}'...\")\n",
    "    video_path = compressed_video\n",
    "    use_webcam = False\n",
    "\n",
    "elif choice == 'w':\n",
    "    print(\"\\nðŸŽ¥ Using webcam.\")\n",
    "    video_path = 0  # Use 0 for the default webcam\n",
    "    use_webcam = True\n",
    "    output_filename = None # No output file for webcam by default\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid choice. Please enter 'v' or 'w'.\")\n",
    "\n",
    "\n",
    "# --- 4. POSE ESTIMATION ---\n",
    "\n",
    "cap = cv2.VideoCapture(video_path,cv2.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(f\"Cannot open video source: {video_path}\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) if not use_webcam else 30\n",
    "\n",
    "# Setup video writer only if processing a video file\n",
    "out = None\n",
    "if not use_webcam:\n",
    "    out = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                          fps, (frame_width, frame_height))\n",
    "\n",
    "with mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=model_complexity_choice,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as pose:\n",
    "\n",
    "    if use_webcam:\n",
    "        print(\"Press 'q' to quit webcam feed.\")\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame from webcam.\")\n",
    "                break\n",
    "\n",
    "            # Flip the frame horizontally for a mirror effect\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Convert the BGR image to RGB\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image_rgb)\n",
    "\n",
    "            # Create a copy to draw on\n",
    "            annotated_image = frame.copy()\n",
    "            if results.pose_landmarks:\n",
    "                annotated_image = draw_body_landmarks(annotated_image, results.pose_landmarks)\n",
    "\n",
    "            cv2.imshow(\"MediaPipe Pose (Webcam)\", annotated_image)\n",
    "            \n",
    "            # Exit loop if 'q' is pressed\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    else: # This block is for video file processing\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f\"\\n--- Step 3: Running Pose Estimation on '{video_path}' ---\")\n",
    "        \n",
    "        with tqdm(total=frame_count, desc=\"Processing Frames\") as pbar:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Convert the BGR image to RGB\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = pose.process(image_rgb)\n",
    "                \n",
    "                # Create a copy to draw on\n",
    "                annotated_image = frame.copy()\n",
    "                if results.pose_landmarks:\n",
    "                    annotated_image = draw_body_landmarks(annotated_image, results.pose_landmarks)\n",
    "                \n",
    "                # Write the frame to the output file\n",
    "                out.write(annotated_image)\n",
    "                pbar.update(1)\n",
    "\n",
    "        print(f\"\\nâœ… Pose estimation complete. Output saved as '{output_filename}'.\")\n",
    "\n",
    "# --- 5. CLEANUP ---\n",
    "cap.release()\n",
    "if out:\n",
    "    out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- 1. Initialization of MediaPipe Pose ---\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# --- 2. Landmark and Connection Definitions ---\n",
    "# Define the landmarks to be excluded (face landmarks)\n",
    "excluded_landmarks = [\n",
    "    mp_pose.PoseLandmark.NOSE, mp_pose.PoseLandmark.LEFT_EYE_INNER,\n",
    "    mp_pose.PoseLandmark.LEFT_EYE, mp_pose.PoseLandmark.LEFT_EYE_OUTER,\n",
    "    mp_pose.PoseLandmark.RIGHT_EYE_INNER, mp_pose.PoseLandmark.RIGHT_EYE,\n",
    "    mp_pose.PoseLandmark.RIGHT_EYE_OUTER, mp_pose.PoseLandmark.LEFT_EAR,\n",
    "    mp_pose.PoseLandmark.RIGHT_EAR, mp_pose.PoseLandmark.MOUTH_LEFT,\n",
    "    mp_pose.PoseLandmark.MOUTH_RIGHT\n",
    "]\n",
    "\n",
    "# Create a list of the 22 body landmarks we are interested in\n",
    "body_landmarks_enum = [lm for lm in mp_pose.PoseLandmark if lm not in excluded_landmarks]\n",
    "\n",
    "# Create a custom set of connections for drawing the skeleton\n",
    "custom_connections = [\n",
    "    connection for connection in mp_pose.POSE_CONNECTIONS\n",
    "    if all(landmark not in excluded_landmarks for landmark in connection)\n",
    "]\n",
    "\n",
    "\n",
    "def normalize_pose_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Applies the full normalization pipeline to a set of pose landmarks.\n",
    "    \n",
    "    Args:\n",
    "        landmarks: A list of landmark objects from MediaPipe.\n",
    "    \n",
    "    Returns:\n",
    "        A flat numpy array of 44 normalized (x, y) coordinates, or None if not possible.\n",
    "    \"\"\"\n",
    "    # 1. Convert landmarks to a NumPy array\n",
    "    # Note: We only use the 22 body landmarks for our calculations and final feature vector.\n",
    "    keypoints = np.array([[landmarks[lm.value].x, landmarks[lm.value].y] for lm in body_landmarks_enum])\n",
    "\n",
    "    # --- Step 1: Filter / Clean (Implicit) ---\n",
    "    # MediaPipe already provides confidence. For a robust pipeline, you would check\n",
    "    # landmark.visibility here and decide how to handle low-confidence points.\n",
    "    # For this script, we assume they are all valid.\n",
    "    \n",
    "    # --- Step 2: Translate (Center) ---\n",
    "    # Use the mid-hip as the root point.\n",
    "    left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "    right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "    root_point = np.array([(left_hip.x + right_hip.x) / 2, (left_hip.y + right_hip.y) / 2])\n",
    "    \n",
    "    keypoints_translated = keypoints - root_point\n",
    "    \n",
    "    # --- Step 3: Scale ---\n",
    "    # Use torso length for normalization.\n",
    "    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "    \n",
    "    mid_shoulder = np.array([(left_shoulder.x + right_shoulder.x) / 2, (left_shoulder.y + right_shoulder.y) / 2])\n",
    "    # We use the translated mid-hip which is now at (0,0) after translation\n",
    "    mid_hip_translated = np.array([(left_hip.x + right_hip.x) / 2, (left_hip.y + right_hip.y) / 2]) - root_point\n",
    "    \n",
    "    # Torso length is the distance between mid-shoulder and mid-hip\n",
    "    torso_length = np.linalg.norm(mid_shoulder - root_point)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if torso_length < 1e-6:\n",
    "        return None\n",
    "        \n",
    "    keypoints_scaled = keypoints_translated / torso_length\n",
    "    \n",
    "    # --- Step 4: Rotate ---\n",
    "    # Align the body so shoulders are horizontal.\n",
    "    # Get the scaled coordinates for the shoulders\n",
    "    left_shoulder_scaled = keypoints_scaled[body_landmarks_enum.index(mp_pose.PoseLandmark.LEFT_SHOULDER)]\n",
    "    right_shoulder_scaled = keypoints_scaled[body_landmarks_enum.index(mp_pose.PoseLandmark.RIGHT_SHOULDER)]\n",
    "    \n",
    "    # Calculate the angle of the shoulder line\n",
    "    shoulder_angle = math.atan2(\n",
    "        right_shoulder_scaled[1] - left_shoulder_scaled[1],\n",
    "        right_shoulder_scaled[0] - left_shoulder_scaled[0]\n",
    "    )\n",
    "    \n",
    "    # Create the rotation matrix for the negative angle\n",
    "    rotation_angle = -shoulder_angle\n",
    "    cos_a = math.cos(rotation_angle)\n",
    "    sin_a = math.sin(rotation_angle)\n",
    "    rotation_matrix = np.array([[cos_a, -sin_a], [sin_a, cos_a]])\n",
    "    \n",
    "    # Apply rotation to all keypoints\n",
    "    keypoints_rotated = np.dot(keypoints_scaled, rotation_matrix.T) # Transpose for correct multiplication\n",
    "    \n",
    "    # --- 5. Flatten to create the final feature vector ---\n",
    "    # Concatenate the [xr_i, yr_i] for all 22 joints.\n",
    "    feature_vector = keypoints_rotated.flatten()\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def process_video(video_path, exercise_name, output_csv_path):\n",
    "    \"\"\"\n",
    "    Processes a video: extracts landmarks, normalizes them, saves to CSV, and shows annotated video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the CSV file exists to write the header only once\n",
    "    file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "    with open(output_csv_path, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write header if the file is new\n",
    "        if not file_exists:\n",
    "            header = ['exercise']\n",
    "            for landmark in body_landmarks_enum:\n",
    "                header += [f'{landmark.name}_x', f'{landmark.name}_y']\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # --- MediaPipe Processing ---\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = pose.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # --- Normalization and Data Saving ---\n",
    "            if results.pose_landmarks:\n",
    "                # Run the normalization pipeline\n",
    "                normalized_landmarks = normalize_pose_landmarks(results.pose_landmarks.landmark)\n",
    "                \n",
    "                if normalized_landmarks is not None:\n",
    "                    # Create the row for the CSV file\n",
    "                    row = [exercise_name] + normalized_landmarks.tolist()\n",
    "                    csv_writer.writerow(row)\n",
    "\n",
    "                # --- Annotation ---\n",
    "                # Create a copy of landmarks to prevent modification of originals\n",
    "                display_landmarks = results.pose_landmarks\n",
    "                # Make face landmarks invisible for drawing\n",
    "                for lm_idx in excluded_landmarks:\n",
    "                    display_landmarks.landmark[lm_idx.value].visibility = 0\n",
    "                \n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    display_landmarks,\n",
    "                    custom_connections, # Use custom connections\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "            cv2.imshow('MediaPipe Pose Annotation', image)\n",
    "\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- INSTRUCTIONS FOR USE ---\n",
    "    # 1. Create a folder named 'videos' in the same directory.\n",
    "    #    Place your exercise videos inside, e.g., 'videos/pushups_1.mp4'.\n",
    "    \n",
    "    # 2. Define the exercises and their corresponding video files.\n",
    "    exercise_videos = {\n",
    "        'pushup': ['videos/pushups_1.mp4', 'videos/pushups_2.mp4'],\n",
    "        'pullup': ['videos/pullups_1.mp4'],\n",
    "        'squat': ['videos/squats_1.mp4']\n",
    "    }\n",
    "\n",
    "    # 3. Specify the name of the output CSV file.\n",
    "    output_csv_file = 'normalized_exercise_landmarks.csv'\n",
    "    \n",
    "    # --- RUN THE PROCESSING ---\n",
    "    for exercise, video_list in exercise_videos.items():\n",
    "        for video_path in video_list:\n",
    "            if os.path.exists(video_path):\n",
    "                print(f\"Processing '{video_path}' for exercise: {exercise}...\")\n",
    "                process_video(video_path, exercise, output_csv_file)\n",
    "            else:\n",
    "                print(f\"Warning: Video file not found at '{video_path}'\")\n",
    "                \n",
    "    print(f\"\\nData processing complete. Normalized landmarks saved to '{output_csv_file}'\")\n",
    "    pose.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionpipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
